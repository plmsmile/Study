{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor和操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1174  0.1588  0.5416\n",
      " 0.7464  0.8802  0.3231\n",
      " 0.2365  0.1080  0.1693\n",
      " 0.3078  0.3747  0.8582\n",
      " 0.7078  0.4216  0.1078\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 未初始化\n",
    "x = torch.Tensor(5, 3)\n",
    "# 随机初始化\n",
    "x = torch.rand(5, 3)\n",
    "print x\n",
    "print x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1, 2])\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.3010  0.7798  1.4748\n",
      " 1.2550  0.8332  1.3546\n",
      " 0.4224  0.8668  0.0814\n",
      " 1.1755  1.3665  1.3029\n",
      " 1.5003  0.6164  1.0745\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 1.3010  0.7798  1.4748\n",
      " 1.2550  0.8332  1.3546\n",
      " 0.4224  0.8668  0.0814\n",
      " 1.1755  1.3665  1.3029\n",
      " 1.5003  0.6164  1.0745\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print (x+y)\n",
    "z = x+y\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6799  0.1859  1.3768\n",
      " 1.6563  0.6146  1.3275\n",
      " 0.4393  0.9025  0.8688\n",
      " 0.6988  1.1053  1.6277\n",
      " 0.9798  0.6091  1.0265\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6799  0.1859  1.3768\n",
      " 1.6563  0.6146  1.3275\n",
      " 0.4393  0.9025  0.8688\n",
      " 0.6988  1.1053  1.6277\n",
      " 0.9798  0.6091  1.0265\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 带有返回值\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6799  0.1859  1.3768\n",
      " 1.6563  0.6146  1.3275\n",
      " 0.4393  0.9025  0.8688\n",
      " 0.6988  1.1053  1.6277\n",
      " 0.9798  0.6091  1.0265\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 直接加在y上面\n",
    "print y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 所有带_的操作都会改变自身，下面都会改变x自身\n",
    "x.copy_(y)\n",
    "x.t_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor和Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print a\n",
    "# torch 转 numpy\n",
    "b = a.numpy()\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "# 改变a, b也随之改变\n",
    "a.add_(1)\n",
    "print (a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numpy to torch\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print 'GPU'\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 3\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# 必须加上\n",
    "x = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "y = Variable(torch.Tensor([3]), requires_grad = True)\n",
    "z = 2 * x + 3 * y + 4\n",
    "# z对x和y进行求导\n",
    "z.backward()\n",
    "# 获得z对x和y的导数\n",
    "print x.grad.data\n",
    "print y.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "# d(out)/dx\n",
    "print x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.1458\n",
      "-1.4115\n",
      "-1.2933\n",
      "-0.7356\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "-0.896536193788\n",
      "2.05604142999\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4)\n",
    "print x\n",
    "print x.mean()\n",
    "# 范数\n",
    "print x.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0100\n",
      " 0.1000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "y = x + 2\n",
    "gradients = torch.FloatTensor([0.01, 0.1, 1])\n",
    "y.backward(gradients)\n",
    "print x.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w:', Parameter containing:\n",
      " 0.2054  0.3019 -0.3009\n",
      "-0.3893  0.2550  0.4192\n",
      "[torch.FloatTensor of size 2x3]\n",
      ")\n",
      "('b:', Parameter containing:\n",
      " 0.4565\n",
      "-0.3095\n",
      "[torch.FloatTensor of size 2]\n",
      ")\n",
      "Variable containing:\n",
      " 0.0471 -0.5359\n",
      " 0.0722 -0.3139\n",
      " 1.2567 -1.4313\n",
      " 1.0304 -0.1997\n",
      " 0.2580 -0.8471\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n",
      "loss: 1.4276945591 1\n",
      "('dl/dw: ', Variable containing:\n",
      " 0.7673 -0.3724 -0.3653\n",
      "-0.2288 -0.0993  0.0386\n",
      "[torch.FloatTensor of size 2x3]\n",
      ")\n",
      "('dl/db: ', Variable containing:\n",
      " 1.0480\n",
      "-0.5449\n",
      "[torch.FloatTensor of size 2]\n",
      ")\n",
      "new loss: 1.40469074249\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# 输入数据 5*3 3维\n",
    "x = Variable(torch.randn(5, 3))\n",
    "y = Variable(torch.randn(5, 2))\n",
    "\n",
    "# 线性层\n",
    "linear = nn.Linear(in_features = 3, out_features = 2, bias = True)\n",
    "print ('w:', linear.weight)\n",
    "print ('b:', linear.bias)\n",
    "\n",
    "# 损失函数\n",
    "loss_func = nn.MSELoss()\n",
    "# 优化方法\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.01)\n",
    "\n",
    "# 计算前向过程\n",
    "pred = linear(x)\n",
    "print pred\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_func(pred, y)\n",
    "print 'loss:', loss.data[0], len(loss.data)\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 输出各变量的梯度\n",
    "print ('dl/dw: ', linear.weight.grad)\n",
    "print ('dl/db: ', linear.bias.grad)\n",
    "\n",
    "# 优化1步\n",
    "optimizer.step()\n",
    "\n",
    "# 用优化后的参数计算前向过程，并计算loss\n",
    "pred = linear(x)\n",
    "loss = loss_func(pred, y)\n",
    "print 'new loss:', loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 激活函数\n",
    "主要是`relu`, `sigmoid`, `tanh`, `softplus`，激活函数输出都是Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# [-5, 5], steps=10, space相同\n",
    "x = torch.linspace(13, 10, steps=10)\n",
    "x = Variable(x)\n",
    "x_np = x.data.numpy()\n",
    "y_relu = F.relu(x).data.numpy()\n",
    "y_sigmoid = F.sigmoid(x).data.numpy()\n",
    "y_tanh = F.tanh(x).data.numpy()\n",
    "y_softplus = F.softplus(x).data.numpy()\n",
    "y_softmax = F.softmax(x).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: ', 0, '| Step: ', 0, '| batch x: ', array([ 6.,  7.,  2.,  3.,  1.]), '| batch y: ', array([  5.,   4.,   9.,   8.,  10.]))\n",
      "('Epoch: ', 0, '| Step: ', 1, '| batch x: ', array([  9.,  10.,   4.,   8.,   5.]), '| batch y: ', array([ 2.,  1.,  7.,  3.,  6.]))\n",
      "('Epoch: ', 1, '| Step: ', 0, '| batch x: ', array([  3.,   4.,   2.,   9.,  10.]), '| batch y: ', array([ 8.,  7.,  9.,  2.,  1.]))\n",
      "('Epoch: ', 1, '| Step: ', 1, '| batch x: ', array([ 1.,  7.,  8.,  5.,  6.]), '| batch y: ', array([ 10.,   4.,   3.,   6.,   5.]))\n",
      "('Epoch: ', 2, '| Step: ', 0, '| batch x: ', array([ 3.,  9.,  2.,  6.,  7.]), '| batch y: ', array([ 8.,  2.,  9.,  5.,  4.]))\n",
      "('Epoch: ', 2, '| Step: ', 1, '| batch x: ', array([ 10.,   4.,   8.,   1.,   5.]), '| batch y: ', array([  1.,   7.,   3.,  10.,   6.]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(1)    \n",
    "\n",
    "BATCH_SIZE = 5\n",
    "x = torch.linspace(1, 10, 10)       # 输入数据\n",
    "y = torch.linspace(10, 1, 10)       # 输出数据\n",
    "\n",
    "# 打包成TensorDataset对象,成为标准数据集\n",
    "torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)\n",
    "# 创建数据加载器\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # TensorDataset类型数据集\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 设置随机洗牌\n",
    "    num_workers=2,              # 加载数据的进程个数\n",
    ")\n",
    "\n",
    "for epoch in range(3):   # 训练3轮\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步\n",
    "        # 在这里写训练代码...\n",
    "        print('Epoch: ', epoch, '| Step: ', step, \n",
    "              '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      " 0.4366 -0.3123 -0.5457\n",
      " 0.0397  0.1968  0.4780\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b:  Parameter containing:\n",
      "1.00000e-02 *\n",
      " -9.5488\n",
      " -4.9416\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bc1187877009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# 预测一下\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# 损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# 清空上一步的梯度缓存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/plm/app/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/plm/app/anaconda2/lib/python2.7/site-packages/torch/nn/modules/linear.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/plm/app/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/plm/app/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/plm/app/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "\n",
    "'''超参'''\n",
    "torch.manual_seed(1)\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "'''准备数据'''\n",
    "#train_data_x = torch.linspace(1, 10, 10)\n",
    "#train_data_y = torch.linspace(10, 1, 10)\n",
    "train_data_x = torch.randn(5, 3)\n",
    "train_data_y = torch.randn(5, 2)\n",
    "\n",
    "# 打包成TensorDataSet对象成为标准数据集\n",
    "torch_dataset = Data.TensorDataset(\n",
    "                data_tensor = train_data_x, target_tensor = train_data_y)\n",
    "# 创建数据加载器\n",
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,     # mini batch size\n",
    "    shuffle = True,\n",
    "    num_workers = 2,               # 加载数据的进程个数\n",
    ")\n",
    "\n",
    "'''构建网络结构'''\n",
    "linear = nn.Linear(in_features = 3, out_features = 2, bias = True)\n",
    "print 'w: ', linear.weight\n",
    "print 'b: ', linear.bias\n",
    "\n",
    "# 如果支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    # 将网络中的参数和缓存移到GPU中\n",
    "    linear = linear.cuda()\n",
    "\n",
    "'''损失函数和优化方法'''\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "'''训练'''\n",
    "for epoch in range(3):\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        if torch.cuda.is_available():\n",
    "            # 将训练数据移至GPU显存\n",
    "            x = Variable(x).cuda()\n",
    "            y = Variable(y).cuda()\n",
    "        output = linear(x)              # 预测一下\n",
    "        loss = loss_func(output, y)     # 损失\n",
    "        optimizer.zero_grad()           # 清空上一步的梯度缓存\n",
    "        loss.backward()                 # 计算新梯度，反向传播\n",
    "        optimizer.step()                # 优化一步\n",
    "        print('Epoch: ', epoch, '| Step: ', step, \n",
    "              '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
