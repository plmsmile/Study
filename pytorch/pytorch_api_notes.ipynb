{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 记录一些PyTorch的常用API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embed1=', torch.Size([4, 10]))\n",
      "('Embed2=', torch.Size([5, 10]))\n",
      "('Embed3=', torch.Size([2, 3, 10]))\n"
     ]
    }
   ],
   "source": [
    "'''Embed的最多数量(词典的数量)，向量的维度'''\n",
    "vocab_size = 10\n",
    "embed_dim = 10\n",
    "# embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embed_dim)\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "input1 = Variable(torch.LongTensor([1, 2, 3, 5]))\n",
    "input2 = Variable(torch.LongTensor([1, 2, 3, 5, 7]))\n",
    "input3 = Variable(torch.LongTensor([[1, 2, 3], [2, 3, 4]]))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Embed1=\", embedding(input1).size())\n",
    "print (\"Embed2=\", embedding(input2).size())\n",
    "print (\"Embed3=\", embedding(input3).size())\n",
    "\n",
    "x = torch.LongTensor([2, 3]).unsqueeze(0)\n",
    "input4 = torch.cat((x, x), 0)\n",
    "#print (input4.size(), input4)\n",
    "#print (\"Embed4\", embedding(input4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('osize=', torch.Size([5, 3, 20]), 'hsize=', torch.Size([2, 3, 20]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 初始化参数\n",
    "(input_size, hidden_size, n_layers) 或者 不选最后一个\n",
    "\n",
    "2. 输入\n",
    "(input, hidden)\n",
    "input: (seq_len, batch, input_size), 一般(1, 1, input_size)\n",
    "hidden: (n_layers*n_dirs, batch, hidden_size), 一般(1, 1, hidden_size)\n",
    "\n",
    "3. 输出\n",
    "(output, hidden)\n",
    "output: 最顶层的RNN输出 (seq_len, batch, hidden_size*n_dirs)，一般(1, b, hidden_size)\n",
    "hidden: 时刻t=seq_len时的隐状态 (n_layers*n_dirs, batch, hidden_size)\n",
    "'''\n",
    "\n",
    "def test1():\n",
    "    rnn = nn.GRU(10, 20, 2)\n",
    "    input = Variable(torch.randn(5, 3, 10))\n",
    "    h0 = Variable(torch.randn(2, 3, 20))\n",
    "    output, h0 = rnn(input, h0)\n",
    "    print ('osize=', output.size(), 'hsize=', h0.size())\n",
    "test1()\n",
    "\n",
    "# 也可以自己实现多层，加个循环就可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### unsqeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([1, 1, 4])\n",
      "torch.Size([1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. torch.unsqueeze 解缩，增加维数。0在前面加维数，1在后面加维数\n",
    "'''\n",
    "#[4]\n",
    "x = torch.Tensor([1, 2, 3, 4])\n",
    "print (x.size())\n",
    "\n",
    "# [1, 4]\n",
    "y1 = torch.unsqueeze(x, 0)\n",
    "print (y1.size())\n",
    "\n",
    "# [4, 1]\n",
    "y2 = torch.unsqueeze(x, 1)\n",
    "print (y2.size())\n",
    "\n",
    "# [1, 1, 4]\n",
    "print (y1.unsqueeze(0).size())\n",
    "# [1, 4, 1]\n",
    "print (y2.unsqueeze(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 4, 5, 1])\n",
      "torch.Size([2, 3, 4, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "移除size=1的维数。0：移除第1维（size=1时移除）。不带参数：移除所有size=1的维数\n",
    "'''\n",
    "def test_squeeze():\n",
    "    x = torch.zeros(2, 1, 3, 4, 5, 1)\n",
    "    # 移除第0维的数据\n",
    "    print (x.squeeze(0).size())\n",
    "    # 默认移除所有size=1的维数\n",
    "    #print (x.squeeze().size())\n",
    "    # 不能移除其它维\n",
    "    print (x.squeeze(1).size())\n",
    "    # 只移除第1维size=1的\n",
    "    x = torch.zeros(1, 1, 2)\n",
    "    #print (x.squeeze(1).size())\n",
    "test_squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tensor中的两个维度做交换\n",
    "'''\n",
    "def test_transpose():\n",
    "    x = torch.randn(2, 3, 4)\n",
    "    print (x.transpose(0, 1).size())\n",
    "    print (x.transpose(0, 2).size())\n",
    "\n",
    "test_transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "(torch.Size([1, 2]), torch.Size([1, 2]), torch.Size([2, 1]))\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "'''dot 需要是1维的tensor'''\n",
    "x = torch.Tensor([2, 3])\n",
    "y = torch.Tensor([2, 1])\n",
    "print (x.dot(y))\n",
    "x = torch.Tensor([[2, 3]])\n",
    "y = torch.Tensor([[2, 3]])\n",
    "print (x.size(), y.size(), y.t().size())\n",
    "# 减到1维可以dot\n",
    "print (x.squeeze(0).dot(y.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bmm 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "'''4. torch.bmm 矩阵相乘的Tensor\n",
    "bach1 b*n*m, batch2: b*m*p, 结果：b*n*p\n",
    "'''\n",
    "batch1 = torch.randn(10, 3, 4)\n",
    "batch2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(batch1, batch2)\n",
    "print (res.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "(<class 'torch.LongTensor'>, <type 'int'>)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "'''topk和max'''\n",
    "x = torch.Tensor([[1,2],[3,4]]) # 5个数 [5]\n",
    "print (x.size())\n",
    "# topk 返回的是(values, indics)\n",
    "values, indics = x[0].topk(1)\n",
    "print (values)\n",
    "print (type(indics), type(indics[0]))\n",
    "# x.max() 会返回最大值，但是不会返回索引\n",
    "print (Variable(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input: T*B*?, T=maxlen, B=batch_size\n",
    "要降序排列, input[:, 0]是最长的, input[:,B-1]是最短的\n",
    "lengths: list[int]\n",
    "\n",
    "output: PackedSequence ps.data可以访问到数据\n",
    "可以用来pack labels, 或者使用RNN的output和它们一起直接计算loss\n",
    "'''\n",
    "\n",
    "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)\n",
    "\n",
    "embedded = self.embedding(input_seqs)\n",
    "# 塞满\n",
    "packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "outputs, hidden = self.gru(packed, hidden)\n",
    "# 拉长\n",
    "outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问3维元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_slice():\n",
    "    a = torch.randn(2, 3, 4)\n",
    "    print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMM矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "'''4. torch.bmm 矩阵相乘的Tensor\n",
    "bach1 b*n*m, batch2: b*m*p, 结果：b*n*p\n",
    "'''\n",
    "batch1 = torch.randn(10, 3, 4)\n",
    "batch2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(batch1, batch2)\n",
    "print (res.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "'''3. torch.cat 把多个tensor组合在一起。形状要统一一样\n",
    "'''\n",
    "x = torch.randn(2, 3)\n",
    "# 按行排列 6*3\n",
    "print (torch.cat((x, x, x), 0).size())\n",
    "# 按列排 2*9\n",
    "print (torch.cat((x, x, x), 1).size())\n",
    "y = torch.randn(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pane_35946a89a899aa'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visdom\n",
    "import numpy as np\n",
    "vis = visdom.Visdom()\n",
    "vis.text('Hello, world!')\n",
    "vis.image(np.ones((3, 10, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
