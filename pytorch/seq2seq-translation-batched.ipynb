{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq translation batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variable(tensor):\n",
    "    ''' 直接获得variable，后面不用在判断，使用GPU或者不使用\n",
    "    '''\n",
    "    var = Variable(tensor)\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用GPU\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言辅助类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang(object):\n",
    "    '''某一语言的辅助类，word2index, index2word, 词频等'''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self, trimmed = False):\n",
    "        '''初始化参数'''\n",
    "        # 修整标记\n",
    "        self.trimmed = trimmed\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"PAD\", 1:\"SOS\", 2:\"EOS\"}\n",
    "        self.n_words = 3\n",
    "    \n",
    "    def index_word(self, word):\n",
    "        '''添加一个词语'''\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def index_sentence(self, sentence, split_str=' '):\n",
    "        '''添加一句话\n",
    "        Args:\n",
    "            sentence: 字符串，单词以空格分割\n",
    "            split_str: 字符串单词分隔符，默认是空格\n",
    "        '''\n",
    "        for word in sentence.split(split_str):\n",
    "            self.index_word(word)\n",
    "    \n",
    "    def index_words(self, words):\n",
    "        '''添加词汇列表\n",
    "        Args:\n",
    "            words: 词汇列表\n",
    "        '''\n",
    "        for word in words:\n",
    "            self.index_word(word)\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        '''移除出现次数太少的单词\n",
    "        Args:\n",
    "            min_count: 最少出现次数\n",
    "        '''\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        keep_words = []\n",
    "        \n",
    "        for word, count in self.word2count.items():\n",
    "            if count >= min_count:\n",
    "                keep_words.append(word)\n",
    "        print (\"keep words: %s / %s = %.4f\" % (len(keep_words), self.n_words,\n",
    "              len(keep_words) / self.n_words))\n",
    "        \n",
    "        # 重新更新参数，重新添加\n",
    "        self.init_params(True)\n",
    "        self.index_words(keep_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转码和规整化字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: Hello你好 %!。 .?!\n",
      "now: hello ! . ? !\n"
     ]
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_str(s):\n",
    "    '''小写化，留下字母和.!?，使用空格分割，删除非法字符'''\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # .!? 前面加空格\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    # 删除非法字符，用空格代替\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    # 多个空格用1个空格代替\n",
    "    s = re.sub(r'\\s+', r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def test_normalize_str():\n",
    "    s = 'Hello你好 %!。 .?!'\n",
    "    sn = normalize_str(s)\n",
    "    print ('raw:', s)\n",
    "    print ('now:', sn)\n",
    "test_normalize_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_lines(filename):\n",
    "#     '''读取filename中的内容，一行一行，转换为ascii码'''\n",
    "#     # io.open\n",
    "#     lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "#     return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# def read_langs(filename, input_name = 'en', target_name ='french', reverse=False):\n",
    "#     ''' 读取\n",
    "#     Args:\n",
    "#         filename: 文件的路径\n",
    "#         input_name: 源语言名称\n",
    "#         target_name: 目标语言名称\n",
    "#         reverse: 是否翻转\n",
    "#     Returns:\n",
    "#         input_lang: 输入语言的对象，只初始化了名字\n",
    "#         target_lang: 输出语言的对象，只初始化了名字\n",
    "#         pairs: [[i1, o1], [i2, o2], ...] 字符串pair\n",
    "#     '''\n",
    "#     lines = read_lines(filename)\n",
    "#     # 每一行以'\\t'分隔两种语言\n",
    "#     pairs = []\n",
    "#     for i, line in enumerate(lines):\n",
    "#         l, r = line.split('\\t')\n",
    "#         l, r = normalize_str(l), normalize_str(r)\n",
    "#         pairs.append([l, r])\n",
    "#     if reverse:\n",
    "#         pairs = [list(reversed(p)) for p in pairs]\n",
    "#         input_lang = Lang(target_name)\n",
    "#         target_lang = Lang(input_name)\n",
    "#     else:\n",
    "#         input_lang = Lang(input_name)\n",
    "#         target_lang = Lang(target_name)\n",
    "#     return input_lang, target_lang, pairs\n",
    "\n",
    "\n",
    "# def test_read_langs():\n",
    "#     '''看下有几个pairs'''\n",
    "#     filename = 'trans_data/en-french.txt'\n",
    "#     input_lang, target_lang, pairs = read_langs(filename)\n",
    "#     print (len(pairs))\n",
    "\n",
    "# # test_read_langs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选数据 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "def legal(sentence):\n",
    "    '''检查句子的长度'''\n",
    "    if len(sentence) >= MIN_LENGTH and len(sentence) <= MAX_LENGTH:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    '''筛选长度合法的pair，两种语言句子都要满足长度'''\n",
    "    remained = []\n",
    "    for p in pairs:\n",
    "        if legal(p[0]) and legal(p[1]):\n",
    "            remained.append(p)\n",
    "    return remained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read: 142787, remain:27065\n",
      "fra:7205, eng:4447\n",
      "[u'est ce ta chambre ?', u'is this your room ?']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(filename, src_name='english', dst_name='french', reverse = False):\n",
    "    ''' 准备数据\n",
    "    Args:\n",
    "        filename: 数据对的文件\n",
    "        src_name: 数据左边的语言\n",
    "        dst_name: 数据右边的语言\n",
    "        reverse: 默认(src-dst), 翻转则(dst-src)\n",
    "    Returns:\n",
    "        input_lang: 源语言 lang对象，name= src_name 或 dst_name (反转) \n",
    "        target_lang: 目标语言 lang对象，name= dst_name 或 src_name (反转) \n",
    "        pairs: [[i1, o1], [i2, o2], [i3, o3], ...]，都是字符串格式\n",
    "    '''\n",
    "    input_lang, target_lang, pairs = read_langs(filename, src_name, dst_name, reverse)\n",
    "    raw_count = len(pairs)\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print ('read: %s, remain:%s' % (raw_count, len(pairs)))\n",
    "    for p in pairs:\n",
    "        input_lang.index_sentence(p[0])\n",
    "        target_lang.index_sentence(p[1])\n",
    "    print ('%s:%s, %s:%s' % (input_lang.name, input_lang.n_words, target_lang.name, target_lang.n_words))\n",
    "    return input_lang, target_lang, pairs\n",
    "\n",
    "def test_read_langs():\n",
    "    '''读取语言数据'''\n",
    "    filename = 'trans_data/en-french.txt'\n",
    "    input_lang, target_lang, pairs = prepare_data(filename, 'eng', 'fra', True)\n",
    "    print (random.choice(pairs))\n",
    "\n",
    "# test_read_langs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read: 142787, remain:27065\n",
      "fra:7205, eng:4447\n",
      "[u'est ce ton parapluie ?', u'is this your umbrella ?']\n"
     ]
    }
   ],
   "source": [
    "filename = 'trans_data/en-french.txt'\n",
    "input_lang, target_lang, pairs = prepare_data(filename, 'eng', 'fra', True)\n",
    "print (random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除出现次数太少的词语\n",
    "\n",
    "便于早点训练完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep words: 1775 / 7205 = 0.2464\n",
      "keep words: 1571 / 4447 = 0.3533\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "input_lang.trim(MIN_COUNT)\n",
    "target_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除包含unknown单词的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs raw:27065, now:16947, 0.6262 remain\n",
      "27065 16947\n"
     ]
    }
   ],
   "source": [
    "keep_pairs = []\n",
    "\n",
    "for p in pairs:\n",
    "    input_sentence = p[0]\n",
    "    target_sentence = p[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "    for word in target_sentence.split(' '):\n",
    "        if word not in target_lang.word2index:\n",
    "            keep_output = False\n",
    "            break    \n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(p)\n",
    "\n",
    "info = 'Pairs raw:%s, now:%s, %.4f remain' % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs))\n",
    "print (info)\n",
    "old_pairs = pairs\n",
    "pairs = keep_pairs\n",
    "print (len(old_pairs), len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据转换成Tensor和Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "为了更好使用GPU，可以一次放多条数据去训练，即一个batch。\n",
    "但是每个句子的长度却是不一样的，比如[2, 3, 4]和[3, 5, 6, 9, 7]。\n",
    "所以需要填充短句子，使每一个batch中所有句子长度一样。计算loss的时候，忽略这些PAD_token\n",
    "有2种解决方案。\n",
    "1. 固定句子的长度，设置MAX_LENGTH，全部句子填充到一样长。\n",
    "2. 短批，长批。短和短为一个batch，长和长为1个batch。\n",
    "这里采用方案1\n",
    "'''\n",
    "\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    ''' 获得句子的词汇的id列表，加上结束标记'''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def pad_seq(seq, max_length):\n",
    "    ''' 为短句子填充到最大长度，填0\n",
    "    Args:\n",
    "        seq: 句子，以词汇id列表来表示\n",
    "        max_length: 要填充到的长度\n",
    "    Returns:\n",
    "        seq: 填充好的句子\n",
    "    '''\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([5, 2]) [5, 5]\n",
      "target: torch.Size([6, 2]) [6, 5]\n"
     ]
    }
   ],
   "source": [
    "def random_batch(batch_size, pairs, input_lang, target_lang):\n",
    "    ''' 随机选择一些样本\n",
    "    Args:\n",
    "        batch_size: 一批的大小\n",
    "        pairs: 原数据\n",
    "        input_lang, target_lang: 两种语言的工具类\n",
    "    Returns:\n",
    "        input_var: [s, b]，即[句子长度，s=句子个数]\n",
    "        input_lengths: 真实长度 [b]\n",
    "        target_var: [s, b]\n",
    "        target_lengths: 真实长度 [b]\n",
    "    '''\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    # 随机选择pairs\n",
    "    for i in range(batch_size):\n",
    "        p = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, p[0]))\n",
    "        target_seqs.append(indexes_from_sentence(target_lang, p[1]))\n",
    "    \n",
    "    # 组合排序再分开\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key = lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # 填充，真实长度，[b, maxlen]\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "    \n",
    "    # LongTensor (seq_len, batch_size)\n",
    "    input_var = get_variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = get_variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    return input_var, input_lengths, target_var, target_lengths\n",
    "\n",
    "def test_random_batch(pairs, input_lang, target_lang):\n",
    "    input_var, in_lens, target_var, t_lens = random_batch(2, pairs, input_lang, target_lang)\n",
    "    print ('input:', input_var.size(), in_lens)\n",
    "    print ('target:', target_var.size(), t_lens)\n",
    "\n",
    "test_random_batch(pairs, input_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    ''' 对句子进行编码 input-embeded-gru-output \n",
    "    [s, batch_size] -- [s, b, h]，即[句子长度，句子个数] -- [句子长度，句子个数，编码维数]\n",
    "    '''\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1, dropout_p=0.1, bidir=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.bidir = bidir\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, \n",
    "                          dropout=dropout_p, bidirectional=bidir)\n",
    "    \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        ''' 对输入的多个句子经过GRU计算出语义信息\n",
    "        1. input_seqs > embeded\n",
    "        2. embeded - packed > GRU > outputs - pad -output\n",
    "        Args:\n",
    "            input_seqs: [s, b]\n",
    "            input_lengths: list[int]，每个句子的真实长度\n",
    "        Returns:\n",
    "            outputs: [s, b, h]\n",
    "            hidden: [n_layer*n_dir, b, h]\n",
    "        '''\n",
    "        # 一次运行，多个batch，多个序列\n",
    "        # print ('inputseqs:', input_seqs.size())\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_length = nn.utils.rnn.pad_packed_sequence(outputs)  \n",
    "        if self.bidir is True:\n",
    "            # 双向，求和\n",
    "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([6, 2]) [6, 4]\n",
      "target: torch.Size([6, 2]) [6, 5]\n",
      "EncoderRNN (\n",
      "  (embedding): Embedding(1778, 8)\n",
      "  (gru): GRU(8, 8, num_layers=2, dropout=0.1)\n",
      ")\n",
      "outputs: torch.Size([6, 2, 8]) hidden: torch.Size([2, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 2\n",
    "input_batches, input_lengths, target_batches, target_lengths \\\n",
    "    = random_batch(small_batch_size, pairs, input_lang, target_lang)\n",
    "print ('input:', input_batches.size(), input_lengths)\n",
    "print ('target:', target_batches.size(), target_lengths)\n",
    "\n",
    "small_hidden_size = 8\n",
    "small_n_layers = 2\n",
    "encoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers, bidir=False)\n",
    "print (encoder_test)\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths)\n",
    "print ('outputs:', encoder_outputs.size(), 'hidden:', encoder_hidden.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    '''计算对齐向量'''\n",
    "    def __init__(self, score_type, hidden_size):\n",
    "        '''\n",
    "        Args:\n",
    "            score_type: 计算score的方法，'dot', 'general', 'concat'\n",
    "            hidden_size: Encoder和Decoder的hidden_size\n",
    "        '''\n",
    "        super(Attn, self).__init__()\n",
    "        self.score_type = score_type\n",
    "        self.hidden_size = hidden_size\n",
    "        if score_type == 'general':\n",
    "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        elif score_type == 'concat':\n",
    "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        ''' 计算Decoder中LSTM的ht与Encoder中的hs的得分，便于后面算对齐概率\n",
    "        Args:\n",
    "            hidden: Decoder中最顶层LSTM的隐状态，[n_layer*n_dir, b, h]\n",
    "            encoder_output: Encoder某时刻的隐状态，h_en_s，[1, h_size]\n",
    "        Returns:\n",
    "            energy: d_ht与e_hs的得分，即Yt与Xs的得分\n",
    "        '''\n",
    "        # dot 需要两个1维的向量\n",
    "        if self.score_type == 'dot':\n",
    "            energy = hidden.squeeze(0).dot(encoder_output.squeeze(0))\n",
    "        elif self.score_type == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            #print('energy:', energy.size(), 'hidden:', hidden.size())\n",
    "            energy = hidden.squeeze(0).dot(energy.squeeze(0))\n",
    "        elif self.score_type == 'concat':\n",
    "            h_o = torch.cat((hidden, encoder_output), 1)\n",
    "            energy = self.attn(h_o)\n",
    "            energy = self.v.squeeze(0).dot(energy.squeeze(0))\n",
    "        return energy\n",
    "    \n",
    "    def forward(self, rnn_output, encoder_outputs):\n",
    "        ''' 时刻t，计算对齐向量\n",
    "        Args:\n",
    "            rnn_output: Decoder中GRU的输出[1, b, h]\n",
    "            encoder_outputs: Encoder的输出, [s_i, b, h]\n",
    "        Returns:\n",
    "            align_vec: 当前ht与所有encoder_outputs的对齐向量，alpha_t，len=Tx，返回[1, 1, seq_len]格式\n",
    "        '''\n",
    "        seq_len = encoder_outputs.size()[0]\n",
    "        this_batch_size = encoder_outputs.size()[1]\n",
    "        # (b,h)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        # attn_energies b*max_len\n",
    "        attn_energies = get_variable(torch.zeros(this_batch_size, seq_len))\n",
    "        for i in range(this_batch_size):\n",
    "            # 依次计算能量\n",
    "            for j in range(max_len):\n",
    "                batch_output = rnn_output[i]\n",
    "                batch_encoder_output = encoder_outputs[:, i, j]\n",
    "                attn_energies[i, j] = self.score(rnn_output[i], encoder_outputs[:, i, j].unsqueeze())\n",
    "        \n",
    "        \n",
    "        \n",
    "        seq_len = len(encoder_outputs)\n",
    "        attn_energies = get_variable(torch.zeros(seq_len))\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "        # normalize [0, 1], resize to [1, 1, seq_len]\n",
    "        align_vec = F.softmax(attn_energies)\n",
    "        align_vec = align_vec.unsqueeze(0).unsqueeze(0)\n",
    "        #print ('alignv:', type(align_vec))\n",
    "        return align_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, score_method='general', n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.score_method = score_method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.contcat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # 选择attention\n",
    "        if score_type != 'none':\n",
    "            self.attn = Attn(score_method, hidden_size)\n",
    "    \n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        1. input > embedded \n",
    "        2. embedded, last_hidden -GRU- rnn_output, hidden\n",
    "        3. rnn_output, encoder_outpus -Atn- attn_weights\n",
    "        4. attn_weights, encoder_outputs -相乘- context\n",
    "        5. rnn_output, context --变换,tanh,变换-- output \n",
    "        Args:\n",
    "            input_seq: [b, o] 上一个的输出单词\n",
    "            last_hidden: [n_layers, b, h]\n",
    "            encoder_outputs: [s_i, b, h]\n",
    "        Returns:\n",
    "            \n",
    "        '''\n",
    "        batch_size = input_seq.size()[0]\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size)\n",
    "        \n",
    "        # (1, b, h), (n_l, b, h)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        # attention\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "    \n",
    "    def init_outputs(self, seq_len, batch_size):\n",
    "        outputs = torch.zeros(seq_len, batch_size, self.output_size)\n",
    "        return get_varaible(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, \n",
    "         encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "         loss_func, batch_size, max_length = MAX_LENGTH):\n",
    "    # zero grad\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # batch_size = input_batches.size()[1]\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    decoder_input = get_variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    \n",
    "    tar_seq_len = max(target_lengths)\n",
    "    # hidden 用encoder的前n层的hidden\n",
    "    decoder_hidden = encoder_hidden(:decoder.n_layers)\n",
    "    decoder_outputs = decoder.init_outputs(tar_seq_len, batch_size)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[u'a', u'b', u'c', u'd']\n",
      "[(u'a', 1), (u'b', 2), (u'c', 3)]\n",
      "[(u'a', u'b', u'c'), (1, 2, 3)]\n",
      "[u'a', u'a', u'a']\n"
     ]
    }
   ],
   "source": [
    "d = {\"a\":\"aa\"}\n",
    "print (\"b\" in d)\n",
    "s = 'a1b1c1d'\n",
    "print (s.split('1'))\n",
    "\n",
    "# zip\n",
    "a = ['a', 'b', 'c']\n",
    "b = [1, 2, 3]\n",
    "# 配对\n",
    "c = zip(a, b)\n",
    "print (c)\n",
    "# 解开\n",
    "print (zip(*c))\n",
    "print (['a'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = get_variable(torch.randn(1, 1, 2))\n",
    "def test(a):\n",
    "    a = a.squeeze()\n",
    "    print (a.size())\n",
    "test(a)\n",
    "print (a.size())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
