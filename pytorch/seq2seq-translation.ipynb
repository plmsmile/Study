{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "data_dir = 'trans_data'\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单词管理的语言类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Lang(object):\n",
    "#     '''某一语言的类，单词-编码，词频统计'''\n",
    "    \n",
    "#     def __init__(self, name):\n",
    "#         self.name = name\n",
    "#         self.word2index = {}\n",
    "#         self.word2count = {}\n",
    "#         self.index2word = {0: 'SOS', 1: \"EOS\"}\n",
    "#         self.n_words = 2\n",
    "    \n",
    "#     def index_words(self, sentence):\n",
    "#         ''' 为语言类添加一句话\n",
    "#         Args:\n",
    "#             sentence: 话，字符串，经过处理后的，以空格分割\n",
    "#         '''\n",
    "#         for word in sentence.split(' '):\n",
    "#             self.index_word(word)\n",
    "    \n",
    "#     def index_word(self, word):\n",
    "#         '''添加一个词汇'''\n",
    "#         if word not in self.word2index:\n",
    "#             idx = self.n_words\n",
    "#             self.word2index[word] = idx\n",
    "#             self.word2count[word] = 1\n",
    "#             self.index2word[idx] = word\n",
    "#             self.n_words += 1\n",
    "#         else:\n",
    "#             self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lang(object):\n",
    "    '''某一语言的辅助类，word2index, index2word, 词频等'''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self, trimmed = False):\n",
    "        '''初始化参数'''\n",
    "        # 修整标记\n",
    "        self.trimmed = trimmed\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"PAD\", 1:\"SOS\", 2:\"EOS\"}\n",
    "        self.n_words = 3\n",
    "    \n",
    "    def index_word(self, word):\n",
    "        '''添加一个词语'''\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def index_sentence(self, sentence, split_str=' '):\n",
    "        '''添加一句话\n",
    "        Args:\n",
    "            sentence: 字符串，单词以空格分割\n",
    "            split_str: 字符串单词分隔符，默认是空格\n",
    "        '''\n",
    "        for word in sentence.split(split_str):\n",
    "            self.index_word(word)\n",
    "    \n",
    "    def index_words(self, words):\n",
    "        '''添加词汇列表\n",
    "        Args:\n",
    "            words: 词汇列表\n",
    "        '''\n",
    "        for word in words:\n",
    "            self.index_word(word)\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        '''移除出现次数太少的单词\n",
    "        Args:\n",
    "            min_count: 最少出现次数\n",
    "        '''\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        keep_words = []\n",
    "        \n",
    "        for word, count in self.word2count.items():\n",
    "            if count >= min_count:\n",
    "                keep_words.append(word)\n",
    "        print (\"keep words: %s / %s = %.4f\" % (len(keep_words), self.n_words,\n",
    "              len(keep_words) / self.n_words))\n",
    "        \n",
    "        # 重新更新参数，重新添加\n",
    "        self.init_params(True)\n",
    "        self.index_words(keep_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转码和规整化字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nihao你好，&&~~··你 在 干嘛呀！。干。。\n",
      "nihao你好 ， 你 在 干嘛呀 ！ 。干 。 。\n"
     ]
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_str(s):\n",
    "    '''小写化，去首尾空格，去掉特殊字符，给标点符号加上空格'''\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # 给.!? 前面加上空格\n",
    "    s = re.sub(r\"([.!?！。，])\", r\" \\1\", s)\n",
    "    # 把非字母和.!?的字符串用空格代替\n",
    "    s = re.sub(r\"[^a-zA-Z.!！。，?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalize_str2(s):\n",
    "    '''小写化，去首尾空格，去掉特殊字符，给标点符号加上空格'''\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # 标点\n",
    "    s = re.sub(r\"([.!?！。，])\", r\" \\1\", s)\n",
    "    s = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z.,!?。，！？ ]', r' ', s)\n",
    "    #print (s)\n",
    "    s = re.sub(r'\\s+', r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def test_normalize_str():\n",
    "    s = 'nihao你好，&&~~··你 在 干嘛呀！。干。。'\n",
    "    print (s)\n",
    "    sn = normalize_str2(s)\n",
    "    print (sn)\n",
    "test_normalize_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    '''读取filename中的内容，一行一行，转换为ascii码'''\n",
    "    # io.open\n",
    "    lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "\n",
    "def read_langs(filename, src_name = 'en', dst_name ='french', reverse=False):\n",
    "    ''' 读取\n",
    "    Args:\n",
    "        filename: 文件的路径\n",
    "        src_name: 源语言名称\n",
    "        dst_name: 目标语言名称\n",
    "        reverse: 是否翻转\n",
    "    Returns:\n",
    "        input_lang: 输入语言的对象，只初始化了名字\n",
    "        target_lang: 输出语言的对象，只初始化了名字\n",
    "        pairs: [[i1, o1], [i2, o2], ...] 字符串pair\n",
    "    '''\n",
    "    lines = read_lines(filename)\n",
    "    # 每一行以'\\t'分隔两种语言\n",
    "    pairs = []\n",
    "    for i, line in enumerate(lines):\n",
    "        l, r = line.split('\\t')\n",
    "        l, r = normalize_str(l), normalize_str(r)\n",
    "        pairs.append([l, r])\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(dst_name)\n",
    "        target_lang = Lang(src_name)\n",
    "    else:\n",
    "        input_lang = Lang(src_name)\n",
    "        target_lang = Lang(dst_name)\n",
    "    return input_lang, target_lang, pairs\n",
    "\n",
    "\n",
    "def test_read_langs():\n",
    "    '''看下有几个pairs'''\n",
    "    filename = 'trans_data/en-french.txt'\n",
    "    input_lang, target_lang, pairs = read_langs(filename)\n",
    "    print (len(pairs))\n",
    "\n",
    "#test_read_langs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择简单数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 句子长度\n",
    "MAX_LENGTH = 10\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def is_simple(pair, en_idx):\n",
    "    ''' 选择简单的句子\n",
    "    长度小于MAX_LENGTH。格式，英语满足上面的前缀格式\n",
    "    Args:\n",
    "        pair: [en_sentence, otherlang_sentence]\n",
    "        en_idx: 英语所在的pair的位置，比如0或者1\n",
    "    '''\n",
    "    src, dst = pair[0].split(' '), pair[1].split(' ')\n",
    "    return (len(src) < MAX_LENGTH and len(dst) < MAX_LENGTH \n",
    "            and pair[en_idx].startswith(good_prefixes))\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, en_idx):\n",
    "    return [p for p in pairs if is_simple(p, en_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(filename, src_name='english', dst_name='french', reverse = False):\n",
    "    ''' 准备数据\n",
    "    Args:\n",
    "        filename: 数据对的文件\n",
    "        src_name: 数据左边的语言\n",
    "        dst_name: 数据右边的语言\n",
    "        reverse: 默认(src-dst), 翻转则(dst-src)\n",
    "    Returns:\n",
    "        input_lang: 源语言 lang对象，name= src_name 或 dst_name (反转) \n",
    "        target_lang: 目标语言 lang对象，name= dst_name 或 src_name (反转) \n",
    "        pairs: [[i1, o1], [i2, o2], [i3, o3], ...]，都是字符串格式\n",
    "    '''\n",
    "    input_lang, target_lang, pairs = read_langs(filename, src_name, dst_name, reverse)\n",
    "    en_idx = 0 if reverse is not True else 1\n",
    "    print (\"read %s lines\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs, en_idx)\n",
    "    print (\"remain %s lines\" % len(pairs))\n",
    "    for p in pairs:\n",
    "        input_lang.index_words(p[0])\n",
    "        target_lang.index_words(p[1])\n",
    "    print (input_lang.name, input_lang.n_words)\n",
    "    print (target_lang.name, target_lang.n_words)\n",
    "    return input_lang, target_lang, pairs\n",
    "\n",
    "def test_read_langs():\n",
    "    '''读取语言数据'''\n",
    "    filename = 'trans_data/en-french.txt'\n",
    "    input_lang, target_lang, pairs = prepare_data(filename, 'eng', 'fra', True)\n",
    "    print (random.choice(pairs))\n",
    "\n",
    "#test_read_langs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename = 'trans_data/en-french.txt'\n",
    "# input_lang, target_lang, pairs = prepare_data(filename, 'eng', 'fra', True)\n",
    "# langs = (input_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取ai数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read en: 5000\n",
      "read en: 10000\n",
      "read en: 15000\n",
      "read en: 20000\n",
      "finish: en: 20000\n",
      "read zn: 5000\n",
      "read zn: 10000\n",
      "read zn: 15000\n",
      "read zn: 20000\n",
      "finish zh: 20000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "en_file = '20170912/seg_en'\n",
    "zh_file = '20170912/seg_zh'\n",
    "\n",
    "input_lang = Lang('english')\n",
    "target_lang = Lang('french')\n",
    "\n",
    "enf = open(en_file, 'r')\n",
    "zhf = open(zh_file, 'r')\n",
    "\n",
    "\n",
    "pairs = []\n",
    "idx = 0\n",
    "total = 20000\n",
    "print_every = 5000\n",
    "for line in enf.readlines():\n",
    "    line = normalize_str2(line)\n",
    "    words = line.split(' ')\n",
    "    input_lang.index_words(words)\n",
    "    pairs.append([line])\n",
    "    idx +=1\n",
    "    if idx % print_every == 0:\n",
    "        print ('read en:', idx)\n",
    "    if idx == total:\n",
    "        break\n",
    "print ('finish: en:', idx)\n",
    "idx = 0\n",
    "for line in zhf.readlines():\n",
    "    line = normalize_str2(line)\n",
    "    words = line.split(' ')\n",
    "    target_lang.index_words(words)\n",
    "    pairs[idx].append(line)\n",
    "    idx += 1\n",
    "    if idx % print_every == 0:\n",
    "        print ('read zn:', idx)\n",
    "    if idx == total:\n",
    "        break\n",
    "print ('finish zh:', idx)\n",
    "pairs_bak = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 27\n",
      "Counter({7: 2016, 8: 2007, 9: 1981, 10: 1943, 6: 1898, 11: 1621, 12: 1491, 5: 1330, 13: 1181, 14: 915, 4: 906, 15: 753, 16: 545, 3: 354, 17: 350, 18: 247, 19: 166, 20: 103, 21: 54, 2: 43, 22: 32, 23: 24, 24: 16, 26: 9, 25: 8, 1: 3, 28: 2, 27: 1, 34: 1})\n",
      "Counter({7: 2409, 6: 2307, 8: 2266, 5: 2215, 9: 1934, 10: 1566, 4: 1488, 11: 1416, 12: 1047, 3: 830, 13: 805, 14: 548, 15: 375, 16: 251, 17: 155, 2: 142, 18: 104, 19: 60, 21: 24, 20: 21, 22: 18, 1: 7, 23: 6, 24: 2, 25: 2, 26: 1, 27: 1})\n"
     ]
    }
   ],
   "source": [
    "# 统计句子长度的分布\n",
    "en_lens = [ len(p[0].split(' ')) for p in pairs]\n",
    "zh_lens = [ len(p[1].split(' ')) for p in pairs]\n",
    "print (max(en_lens), max(zh_lens))\n",
    "en_lens_c = Counter(en_lens)\n",
    "zh_lens_c = Counter(zh_lens)\n",
    "print (en_lens_c)\n",
    "print (zh_lens_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34, 1), (28, 2), (27, 1), (26, 9), (25, 8), (24, 16), (23, 24), (22, 32), (21, 54), (20, 103), (19, 166), (18, 247), (17, 350), (16, 545), (15, 753), (14, 915), (13, 1181), (12, 1491), (11, 1621), (10, 1943), (9, 1981), (8, 2007), (7, 2016), (6, 1898), (5, 1330), (4, 906), (3, 354), (2, 43), (1, 3)]\n",
      "[(27, 1), (26, 1), (25, 2), (24, 2), (23, 6), (22, 18), (21, 24), (20, 21), (19, 60), (18, 104), (17, 155), (16, 251), (15, 375), (14, 548), (13, 805), (12, 1047), (11, 1416), (10, 1566), (9, 1934), (8, 2266), (7, 2409), (6, 2307), (5, 2215), (4, 1488), (3, 830), (2, 142), (1, 7)]\n",
      "en: 22 zh: 19\n"
     ]
    }
   ],
   "source": [
    "en_min_count = 30\n",
    "zh_min_count = 30\n",
    "en_max_len = 100\n",
    "zh_max_len = 100\n",
    "\n",
    "en_d = dict(en_lens_c)\n",
    "zh_d = dict(zh_lens_c)\n",
    "en_d = sorted(en_d.items(), key = lambda d: d[0], reverse=True)\n",
    "zh_d = dict(zh_lens_c)\n",
    "zh_d = sorted(zh_d.items(), key = lambda d: d[0], reverse=True)\n",
    "print (en_d)\n",
    "print (zh_d)\n",
    "for l, c in en_d:\n",
    "    if c >= en_min_count:\n",
    "        en_max_len = l\n",
    "        break\n",
    "\n",
    "for l, c in zh_d:\n",
    "    if c >= zh_min_count:\n",
    "        zh_max_len = l\n",
    "        break\n",
    "\n",
    "print ('en:', en_max_len, 'zh:', zh_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 - 19884, 0.9942\n"
     ]
    }
   ],
   "source": [
    "def filter_pairs(pairs):\n",
    "    '''筛选长度合法的pair，两种语言句子都要满足长度'''\n",
    "    remained = []\n",
    "    for p in pairs:\n",
    "        if len(p[0].split(' ')) <= en_max_len and len(p[1].split(' ')) <= zh_max_len:\n",
    "            remained.append(p)\n",
    "    return remained\n",
    "new_pairs = filter_pairs(pairs)\n",
    "print ('%s - %s, %.4f' % (len(pairs), len(new_pairs), len(new_pairs) / len(pairs)))\n",
    "pairs = new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep words: 5762 / 12187 = 0.4728\n",
      "Pairs raw:19884, now:14791, 0.7439 remain\n",
      "19884 14791\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2\n",
    "input_lang.trim(MIN_COUNT)\n",
    "keep_pairs = []\n",
    "for p in pairs:\n",
    "    input_sentence = p[0]\n",
    "    keep_input = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "    if keep_input:\n",
    "        keep_pairs.append(p)\n",
    "\n",
    "info = 'Pairs raw:%s, now:%s, %.4f remain' % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs))\n",
    "print (info)\n",
    "old_pairs = pairs\n",
    "print (len(old_pairs), len(keep_pairs))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n"
     ]
    }
   ],
   "source": [
    "print (pairs[0][0])\n",
    "langs = (input_lang, target_lang)\n",
    "MAX_LENGTH = zh_max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据转换成Tensor和Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "    3\n",
      "   29\n",
      "    5\n",
      "   30\n",
      "   31\n",
      "   32\n",
      "   33\n",
      "   34\n",
      "   35\n",
      "   36\n",
      "   28\n",
      "    1\n",
      "[torch.LongTensor of size 12x1]\n",
      ", Variable containing:\n",
      "    3\n",
      "   26\n",
      "   27\n",
      "   28\n",
      "    9\n",
      "   29\n",
      "   25\n",
      "    1\n",
      "[torch.LongTensor of size 8x1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def indics_from_sentence(lang, sentence):\n",
    "    ''' 获得句子的词汇的id列表\n",
    "    Args:\n",
    "        lang: sentence所在的语言对象\n",
    "        sentence: 一句话\n",
    "    Returns:\n",
    "        [] 句子中单词的id列表\n",
    "    '''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    ''' 获得一个句子的单词id组成的Variable\n",
    "    Rertuns:\n",
    "        indics_variable: 句子的单词的id列表，由Variable表示，(len, 1)\n",
    "    '''\n",
    "    indics = indics_from_sentence(lang, sentence)\n",
    "    indics.append(EOS_token)\n",
    "    # 训练模型，使用Variable作为输入，因为Variable可以跟踪计算图，自动计算梯度\n",
    "    indics_variable = Variable(torch.LongTensor(indics).view(-1, 1))\n",
    "    if USE_CUDA:\n",
    "        indics_variable = indics_variable.cuda()\n",
    "    return indics_variable\n",
    "\n",
    "\n",
    "def variables_from_pair(langs, pair):\n",
    "    '''\n",
    "    Args:\n",
    "        langs: 两个语言对象List，(input_lang, target_lang)\n",
    "        pair: 一组句子list，(输入句子, 目标句子)\n",
    "    '''\n",
    "    input_variable = variable_from_sentence(langs[0], pair[0])\n",
    "    target_variable = variable_from_sentence(langs[1], pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n",
    "def test_variables_from_pair():\n",
    "    '''选择一个pair，打印出相应的句子的单词id列表'''\n",
    "    pair_var = variables_from_pair(langs, pairs[2])\n",
    "    print (pair_var)\n",
    "\n",
    "test_variables_from_pair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variable(tensor):\n",
    "    ''' 直接获得variable，后面不用在判断，使用GPU或者不使用\n",
    "    '''\n",
    "    var = Variable(tensor)\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    '''input-embed-gru'''\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        '''\n",
    "        Args:\n",
    "            vocab_size: 输入语言的词汇表大小\n",
    "            hidden_size: GRU的输入输出维数，词向量的维数\n",
    "            n_layers: GRU的层数\n",
    "        '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "    \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        '''\n",
    "        1. word_inputs > embedded\n",
    "        2. embedded > GRU > output, hidden\n",
    "        Args:\n",
    "            word_inputs: 输入的单词id列表，可以是1个或者多个\n",
    "            hidden: [n_layers, 1, hidden_size]\n",
    "        Returns:\n",
    "            output: [seq_len, 1, h_size]\n",
    "            hidden: [n_layers, 1, h_size]\n",
    "        '''\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        '''初始化hidden'''\n",
    "        hidden = get_variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    '''计算对齐向量'''\n",
    "    def __init__(self, score_type, hidden_size, max_length = MAX_LENGTH):\n",
    "        '''\n",
    "        Args:\n",
    "            score_type: 计算score的方法，'dot', 'general', 'concat'\n",
    "            hidden_size: Encoder和Decoder的hidden_size\n",
    "            max_length: \n",
    "        '''\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.score_type = score_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        if score_type == 'general':\n",
    "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        elif score_type == 'concat':\n",
    "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        ''' 计算Decoder中LSTM的ht与Encoder中的hs的得分，便于后面算对齐概率\n",
    "        Args:\n",
    "            hidden: Decoder中最顶层LSTM的隐状态，h_de_t，[1, h_size]\n",
    "            encoder_output: Encoder某时刻的隐状态，h_en_s，[1, h_size]\n",
    "        Returns:\n",
    "            energy: d_ht与e_hs的得分，即Yt与Xs的得分\n",
    "        '''\n",
    "        # dot 需要两个1维的向量\n",
    "        if self.score_type == 'dot':\n",
    "            energy = hidden.squeeze(0).dot(encoder_output.squeeze(0))\n",
    "        elif self.score_type == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            #print('energy:', energy.size(), 'hidden:', hidden.size())\n",
    "            energy = hidden.squeeze(0).dot(energy.squeeze(0))\n",
    "        elif self.score_type == 'concat':\n",
    "            h_o = torch.cat((hidden, encoder_output), 1)\n",
    "            energy = self.attn(h_o)\n",
    "            energy = self.other.squeeze(0).dot(energy.squeeze(0))\n",
    "        return energy\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        ''' 时刻t，计算对齐向量\n",
    "        Args:\n",
    "            hidden: Top LSTM的隐状态\n",
    "            encoder_outputs: Encoder的所有隐状态, len=Tx输入句子长度\n",
    "        Returns:\n",
    "            align_vec: 当前ht与所有encoder_outputs的对齐向量，alpha_t，len=Tx，返回[1, 1, seq_len]格式\n",
    "        '''\n",
    "        seq_len = len(encoder_outputs)\n",
    "        attn_energies = get_variable(torch.zeros(seq_len))\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "        # normalize [0, 1], resize to [1, 1, seq_len]\n",
    "        align_vec = F.softmax(attn_energies)\n",
    "        align_vec = align_vec.unsqueeze(0).unsqueeze(0)\n",
    "        #print ('alignv:', type(align_vec))\n",
    "        return align_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttnDecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, score_type='general', dropout_p=0.1):\n",
    "        ''' init\n",
    "        Args:\n",
    "            hidden_size: Embedding和GRU的hidden_size\n",
    "            output_size: 输出的size，是目标语言的词典大小\n",
    "            score_type: 'dot', 'general', 'concat'\n",
    "            n_layers: GRU的层数\n",
    "            dropout_p: dropout\n",
    "        '''\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.score_type = score_type\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # 注意input_size\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        # 计算对齐向量\n",
    "        self.attn = Attn(score_type, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        1. (word_embedded, last_context) > rnn_input\n",
    "        2. rnn_input, last_hidden > GRU > rnn_output, hidden\n",
    "        3. rnn_output, encoder_outputs > Attn > attn_weights\n",
    "        4. attn_weights * encoder_outputs > context\n",
    "        5. (rnn_output, context) > Out > Softmax > output\n",
    "        Args:\n",
    "            word_input: word_id\n",
    "            last_context: [1, h]\n",
    "            last_hidden: [1, 1, h]\n",
    "            encoder_outputs: [seq_len, 1, h]\n",
    "        Returns:\n",
    "            output: [1, o_size]\n",
    "            context: [1, h]\n",
    "            hidden: [1, 1, h]\n",
    "            attn_weights: [1, 1, inseq_len]\n",
    "        '''\n",
    "        # 当前input_word, 实际上是last output_word\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)\n",
    "        \n",
    "        # 串联input_word和last_context作为GRU的输入\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        # 当前输入一个单词 (seq_len=1, b, h) (n_layers*n_dirs, b, h)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # 计算output与encoder_outputs的对齐向量 (1, 1, seq_len)\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        #print (type(attn_weights), attn_weights)\n",
    "        # 语义向量 (1, 1, seq_len) (1, seq_len, h) > (1, 1, h)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "        # RNN h和context 输出output (1,1,h) > (1, h)\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        # (1, 1, h) > (1, h)\n",
    "        context = context.squeeze(1)\n",
    "        # (1, 2h)\n",
    "        ro_c = torch.cat((rnn_output, context), 1)\n",
    "        # (1, o_size)\n",
    "        output = F.log_softmax(self.out(ro_c))\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(10, 10, num_layers=2)\n",
      ")\n",
      "AttnDecoderRNN (\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(20, 10, num_layers=2, dropout=0.1)\n",
      "  (out): Linear (20 -> 10)\n",
      "  (attn): Attn (\n",
      "    (attn): Linear (10 -> 10)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    encoder = EncoderRNN(10, 10, 2)\n",
    "    decoder = AttnDecoderRNN(10, 10, 2, score_type='general')\n",
    "    print (encoder)\n",
    "    print (decoder)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    word_inputs = get_variable(torch.LongTensor([1, 2, 3]))\n",
    "    encoder_outputs, encoder_hidden = encoder(word_inputs, encoder_hidden)\n",
    "    #print ('outputs:', encoder_outputs.size(), ', hidden:', encoder_hidden.size())\n",
    "    \n",
    "    attns = torch.zeros(1, 3, 3)\n",
    "    hidden = encoder_hidden\n",
    "    context = get_variable(torch.zeros(1, decoder.hidden_size))\n",
    "    \n",
    "    for i in range(3):\n",
    "        output, context, hidden, attn_weights = decoder(word_inputs[i], context, hidden, encoder_outputs)\n",
    "        print (output.size(), hidden.size(), attn_weights.size())\n",
    "    \n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.4\n",
    "clip = 5.0\n",
    "\n",
    "def train(inputv, targetv, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, loss_func, max_length = MAX_LENGTH):\n",
    "    # zero grad\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # 输入和输出句子长度\n",
    "    input_len = inputv.size()[0]\n",
    "    target_len = targetv.size()[0]\n",
    "    \n",
    "    # encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(inputv, encoder_hidden)\n",
    "    \n",
    "    # input and output variables\n",
    "    decoder_input = get_variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = get_variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # 使用真实值作为输入\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = \\\n",
    "                decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            #print('******************', decoder_output[0].size(), targetv[i].size())\n",
    "            loss += loss_func(decoder_output, targetv[i]) \n",
    "            decoder_input = targetv[i]\n",
    "    else:\n",
    "        # 使用网络预测的作为输入\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = \\\n",
    "                decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            #print('******************', decoder_output[0].size(), targetv[i].size())\n",
    "            loss += loss_func(decoder_output, targetv[i]) \n",
    "            # 找到最有可能的词\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            decoder_input = get_variable(torch.LongTensor([[ni]]))\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "    # bp\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data[0] / target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return ('%dm %ds') % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN (\n",
      "  (embedding): Embedding(12187, 500)\n",
      "  (gru): GRU(500, 500, num_layers=2)\n",
      ")\n",
      "AttnDecoderRNN (\n",
      "  (embedding): Embedding(18197, 500)\n",
      "  (gru): GRU(1000, 500, num_layers=2, dropout=0.05)\n",
      "  (out): Linear (1000 -> 18197)\n",
      "  (attn): Attn (\n",
      "    (attn): Linear (500 -> 500)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "score_type = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "learning_rate = 0.0001\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(hidden_size, target_lang.n_words, n_layers=n_layers,\n",
    "                         score_type=score_type, dropout_p=dropout_p)\n",
    "print (encoder)\n",
    "print (decoder)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练信息\n",
    "n_epochs = int(50000*1.5)\n",
    "plot_every = int(200*1.5)\n",
    "print_every = int(500)\n",
    "\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0\n",
    "plot_loss_total = 0\n",
    "log_file = './seq2seq-enzh.log'\n",
    "f = open(log_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33m 49s (- 5039m 11s) (500 0%) 5.0958\n",
      "106m 15s (- 5206m 54s) (1500 2%) 4.7210\n",
      "143m 4s (- 5222m 28s) (2000 2%) 4.6681\n",
      "180m 10s (- 5225m 6s) (2500 3%) 4.5910\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'models/en-zh-1015'\n",
    "save_every = 20000\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    training_pair = variables_from_pair(langs, random.choice(pairs))\n",
    "    inputv = training_pair[0]\n",
    "    targetv = training_pair[1]\n",
    "    loss = train(inputv, targetv, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func)\n",
    "    \n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    \n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        f.write(print_summary + '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "    \n",
    "    if epoch % save_every == 0:\n",
    "        no = epoch / save_every\n",
    "        to = n_epochs / save_every\n",
    "        s = '{}_{}'.format(no, to)\n",
    "        torch.save(encoder, model_dir + '/' + s + 'encoder.pkl')\n",
    "        torch.save(encoder, model_dir + '/' + s + 'decoder.pkl')\n",
    "\n",
    "# 保存模型\n",
    "model_dir = 'models/en-zh-1015'\n",
    "torch.save(encoder, model_dir + '/encoder.pkl')\n",
    "torch.save(decoder, model_dir + '/decoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-e75c6bfccc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/encoder.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/decoder.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "encoder1 = torch.load(model_dir + '/encoder.pkl')\n",
    "decoder1 = torch.load(model_dir + '/decoder.pkl')\n",
    "\n",
    "\n",
    "def evaluate(sentence, input_lang, target_lang, encoder, decoder, max_length = MAX_LENGTH):\n",
    "    inputv = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = inputv.size()[0]\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(inputv, encoder_hidden)\n",
    "    \n",
    "    decoder_input = get_variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = get_variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "     # 使用网络预测的作为输入\n",
    "    for i in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = \\\n",
    "            decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        #print('******************', decoder_output[0].size(), targetv[i].size())\n",
    "        # 找到最有可能的词\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_input = get_variable(torch.LongTensor([[ni]]))\n",
    "        decoded_words.append(target_lang.index2word[ni])\n",
    "        if ni == EOS_token:\n",
    "            break\n",
    "    return decoded_words\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, input_lang, target_lang):\n",
    "    pair = random.choice(pairs)\n",
    "    output_words = evaluate(pair[0], input_lang, target_lang, encoder1, decoder1)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print ('raw:', pair[0])\n",
    "    print ('tar:', pair[1])\n",
    "    print ('now:', output_sentence)\n",
    "\n",
    "for i in range(5):\n",
    "    evaluate_randomly(encoder1, decoder1, input_lang, target_lang)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je t'aime\n",
      "now: i m you you . EOS\n",
      "J'ai besoin de toi\n",
      "now: i m on you of you . EOS\n",
      "J'ai faim\n",
      "now: i m hungry hungry . EOS\n",
      "Il avait faim\n",
      "now: he s got hungry . EOS\n",
      "Il est plein de nourriture\n",
      "now: he is full of treachery . EOS\n",
      "Il veut manger\n",
      "now: he s a bad liar . EOS\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    s = raw_input()\n",
    "    s = unicode(s)\n",
    "    s = normalize_str(s)\n",
    "    output_words = evaluate(s, input_lang, target_lang, encoder1, decoder1)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    #print ('raw:', pair[0])\n",
    "    #print ('tar:', pair[1])\n",
    "    print ('now:', output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9cb46db210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9cb3f36d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = open('trans_data/data_10000.pkl', 'rb')\n",
    "bak = pickle.load(output)\n",
    "train_en = bak['train_en']\n",
    "train_len_en = bak['train_len_en']\n",
    "train_zh = bak['train_zh']\n",
    "train_len_zh = bak['train_len_zh']\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[113, 2438, 6, 747, 87447, 11, 87447, 15676, 17514, 17, 20000, 41, 88, 18581, 3308, 338352]\n"
     ]
    }
   ],
   "source": [
    "print (type(train_en))\n",
    "print (train_en[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
